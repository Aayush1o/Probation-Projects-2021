{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Task 2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOQH+rg2/XYSRksfrR4hGML"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LPOCfIOB6efy"},"source":["**Machine Translation model english to spanish**"]},{"cell_type":"markdown","metadata":{"id":"2HxH4AsQ66sN"},"source":["Importing essential Libraries and modules"]},{"cell_type":"code","metadata":{"id":"qisInB1M7A5y","executionInfo":{"status":"ok","timestamp":1638330921580,"user_tz":-330,"elapsed":5933,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}}},"source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"csSEH0Yf7H89"},"source":["Downloading the data"]},{"cell_type":"markdown","metadata":{"id":"QUBX-aSu8Eg4"},"source":["Here i am working with English to spanish Dataset provided by [ANKi](https://https://www.manythings.org/anki/)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_IlvQxXR7mD5","executionInfo":{"status":"ok","timestamp":1638330924090,"user_tz":-330,"elapsed":10,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}},"outputId":"36b2d87a-3441-4a11-8088-b9581212fdf5"},"source":["text_file = keras.utils.get_file(\n","    fname=\"spa-eng.zip\",\n","    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","    extract=True,\n",")\n","text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","2646016/2638744 [==============================] - 0s 0us/step\n","2654208/2638744 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","metadata":{"id":"jJ7cZS_c8oah"},"source":["**Data preprocessing**"]},{"cell_type":"markdown","metadata":{"id":"Rx66eHeM9QKo"},"source":["Each line contains an english sentence then its spanish version so i am going to token [start] for english sentece and [end] for spanish sentence"]},{"cell_type":"code","metadata":{"id":"I6hoW30x6uzN","executionInfo":{"status":"ok","timestamp":1638330925951,"user_tz":-330,"elapsed":3,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}}},"source":["with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    eng, spa = line.split(\"\\t\")\n","    spa = \"[start] \" + spa + \" [end]\"\n","    text_pairs.append((eng, spa))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIo_La3u9fUZ"},"source":["sentence pair look like this "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnLczKgR9kwJ","executionInfo":{"status":"ok","timestamp":1638330930926,"user_tz":-330,"elapsed":441,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}},"outputId":"731c7842-2c43-4554-cbdc-821793b1884b"},"source":["for _ in range(5):\n","    print(random.choice(text_pairs))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(\"I'm surprised you remember Tom.\", '[start] Estoy sorprendido de que te acuerdes de Tom. [end]')\n","('He wore old shoes.', '[start] Él usaba zapatos viejos. [end]')\n","('I put some cream in my coffee.', '[start] Le pongo crema a mi café. [end]')\n","('My sister has made remarkable progress in English.', '[start] Mi hermana ha hecho un progreso notable en inglés. [end]')\n","('Wait until I sit down.', '[start] Espera a que me siente. [end]')\n"]}]},{"cell_type":"markdown","metadata":{"id":"u6P-gqsp93Ao"},"source":["Spliting the data set into training set , validation set and test set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vqaynq0z-DMq","executionInfo":{"status":"ok","timestamp":1638330941180,"user_tz":-330,"elapsed":411,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}},"outputId":"30a2f04f-86d1-4e7e-9d40-0782144fad7a"},"source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["118964 total pairs\n","83276 training pairs\n","17844 validation pairs\n","17844 test pairs\n"]}]},{"cell_type":"markdown","metadata":{"id":"OB8S4b7k-ICc"},"source":["**Vectorizing the data**\n","it means turning original strings into integer sequence.I am using textvectorization"]},{"cell_type":"code","metadata":{"id":"0nUs-1CQ-_7u","executionInfo":{"status":"ok","timestamp":1638330966780,"user_tz":-330,"elapsed":22587,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}}},"source":["strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","eng_vectorization = TextVectorization(\n","    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","spa_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","eng_vectorization.adapt(train_eng_texts)\n","spa_vectorization.adapt(train_spa_texts)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXtsS9s9_SFb"},"source":["**Formating Dataset**\n","At each training step, the model will seek to predict target words N+1 (and beyond) using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple (inputs, targets), where:\n","\n","inputs is a dictionary with the keys encoder_inputs and decoder_inputs. encoder_inputs is the vectorized source sentence and encoder_inputs is the target sentence \"so far\", that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n","target is the target sentence offset by one step: it provides the next words in the target sentence -- what the model will try to predict."]},{"cell_type":"code","metadata":{"id":"eBYcsViM_cxo","executionInfo":{"status":"ok","timestamp":1638330966781,"user_tz":-330,"elapsed":11,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}}},"source":["def format_dataset(eng, spa):\n","    eng = eng_vectorization(eng)\n","    spa = spa_vectorization(spa)\n","    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IsaD0W8U-9V1","executionInfo":{"status":"ok","timestamp":1638295208666,"user_tz":-330,"elapsed":1449,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}},"outputId":"ce116e12-2a2f-4ebc-b14b-26cbb337d540"},"source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs[\"encoder_inputs\"].shape: (64, 20)\n","inputs[\"decoder_inputs\"].shape: (64, 20)\n","targets.shape: (64, 20)\n"]}]},{"cell_type":"markdown","metadata":{"id":"ztbyrKGyAm4n"},"source":["**Building the model**"]},{"cell_type":"markdown","metadata":{"id":"WVxWm489AqrU"},"source":["Our sequence-to-sequence Transformer consists of a TransformerEncoder and a TransformerDecoder chained together. To make the model aware of word order, we also use a PositionalEmbedding layer."]},{"cell_type":"code","metadata":{"id":"OQ3KKslEATZa","executionInfo":{"status":"ok","timestamp":1638330968303,"user_tz":-330,"elapsed":29,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}}},"source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BjcXxRlPA72j"},"source":["Now assemble the end to end model"]},{"cell_type":"code","metadata":{"id":"RNJnw9jlATLA","executionInfo":{"status":"ok","timestamp":1638330978162,"user_tz":-330,"elapsed":1337,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}}},"source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QJ1aS0WfBIT4"},"source":["**Training the model**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3YgpIAtBU3n","executionInfo":{"status":"ok","timestamp":1638331194757,"user_tz":-330,"elapsed":208595,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}},"outputId":"63d1d490-0acd-4e80-a465-9b854a65a7c3"},"source":["epochs = 1  # This should be at least 30 for convergence\n","\n","transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," positional_embedding (Position  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n"," alEmbedding)                                                                                     \n","                                                                                                  \n"," decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n","                                                                                                  \n"," transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n"," erEncoder)                                                                                       \n","                                                                                                  \n"," model_1 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n","                                                                  'transformer_encoder[0][0]']    \n","                                                                                                  \n","==================================================================================================\n","Total params: 19,960,216\n","Trainable params: 19,960,216\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","1302/1302 [==============================] - 175s 128ms/step - loss: 1.6386 - accuracy: 0.4322 - val_loss: 1.3299 - val_accuracy: 0.5099\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7efb3c8e4c10>"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"GL58YqyzBeFa"},"source":["**Decoding test sentences**"]},{"cell_type":"markdown","metadata":{"id":"HWe2rW8cB1Fv"},"source":["Finally, let's demonstrate how to translate brand new English sentences. We simply feed into the model the vectorized English sentence as well as the target token \"[start]\", then we repeatedly generated the next token, until we hit the token \"[end]\"."]},{"cell_type":"code","metadata":{"id":"pOhdFz17B3NY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638331283513,"user_tz":-330,"elapsed":14612,"user":{"displayName":"Aayush Aggarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUuNBb4K_7H5ApncfQKs6L2G8so_wwanceTSTL=s64","userId":"05678384852396543133"}},"outputId":"12451277-c367-4694-af27-cd6cd5582183"},"source":["spa_vocab = spa_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(30):\n","    input_sentence = random.choice(test_eng_texts)\n","    translated = decode_sequence(input_sentence)\n","print(input_sentence) \n","print(translated)   "],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["After a certain point, everything became a little more difficult.\n","[start] después de que se [UNK] a todo más más más más más más más más más rápido [end]\n"]}]}]}